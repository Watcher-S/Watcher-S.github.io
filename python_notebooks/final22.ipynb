{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc2e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing static map at /Users/simon/altair-hw/Watcher-S.github.io/python_notebooks/local_maps/chicago_static_map.png\n"
     ]
    }
   ],
   "source": [
    "# Download a local static Chicago map (optional fallback)\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "MAP_DIR = Path('local_maps')\n",
    "MAP_DIR.mkdir(exist_ok=True)\n",
    "STATIC_MAP_PATH = MAP_DIR / 'chicago_static_map.png'\n",
    "\n",
    "def ensure_static_map(path=STATIC_MAP_PATH, size=(1024, 768), center=(41.8781, -87.6298), zoom=11):\n",
    "    try:\n",
    "        try:\n",
    "            from staticmap import StaticMap, CircleMarker\n",
    "        except Exception:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'staticmap', 'Pillow'])\n",
    "            from staticmap import StaticMap, CircleMarker\n",
    "        width, height = size\n",
    "        m = StaticMap(width, height, url_template='https://tile.openstreetmap.org/{z}/{x}/{y}.png')\n",
    "        # Add a transparent marker just to fix center\n",
    "        cm = CircleMarker((center[1], center[0]), '#00000000', 1)\n",
    "        m.add_marker(cm)\n",
    "        image = m.render(zoom=zoom, center=(center[1], center[0]))\n",
    "        image.save(path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'Could not create static map: {e}')\n",
    "        return False\n",
    "\n",
    "if not STATIC_MAP_PATH.exists():\n",
    "    print('Generating local static map of Chicago...')\n",
    "    ok = ensure_static_map(STATIC_MAP_PATH)\n",
    "    if ok:\n",
    "        print(f'Local static map saved to {STATIC_MAP_PATH.resolve()}')\n",
    "    else:\n",
    "        print('Static map generation skipped.')\n",
    "else:\n",
    "    print(f'Found existing static map at {STATIC_MAP_PATH.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c0864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping local CSV download. Using Chicago SODA API for data loading.\n"
     ]
    }
   ],
   "source": [
    "# Auto-download dataset if missing (DISABLED) — using SODA API instead\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_NAME = 'traffic_crashes_full.csv'\n",
    "CSV_PATH = Path(CSV_NAME)\n",
    "\n",
    "# Ensure requests is available for SODA API calls in the next cell\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'requests'])\n",
    "    import requests\n",
    "\n",
    "# Do not download large CSV; rely on SODA API loader cell.\n",
    "print('Skipping local CSV download. Using Chicago SODA API for data loading.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26744b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading crash data from Chicago SODA API...\n",
      "Loaded 250000 rows from SODA API\n",
      "Loaded 250000 rows from SODA API\n"
     ]
    }
   ],
   "source": [
    "# Prefer Chicago SODA API loader (fallback to local CSV)\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "import math\n",
    "\n",
    "SODA_BASE = 'https://data.cityofchicago.org/resource/85ca-t3if.csv'\n",
    "SODA_JSON = 'https://data.cityofchicago.org/resource/85ca-t3if.json'\n",
    "SODA_APP_TOKEN = os.environ.get('CHICAGO_SODA_TOKEN', None)  # optional\n",
    "\n",
    "# Query params: adjust defaults as needed\n",
    "params = {\n",
    "    '$select': '*',\n",
    "    '$order': 'crash_date DESC',\n",
    "}\n",
    "# Example date filter (comment out to fetch all allowed by limit)\n",
    "# params['$where'] = \"crash_date >= '2024-01-01T00:00:00' AND crash_date <= '2024-12-31T23:59:59'\"\n",
    "\n",
    "DEFAULT_LIMIT = 50000  # per page\n",
    "MAX_ROWS = 250000      # total cap to keep memory reasonable\n",
    "\n",
    "use_soda = True  # set False to force local CSV usage\n",
    "csv_path = 'traffic_crashes_full.csv'\n",
    "\n",
    "\n",
    "def soda_fetch_json(params, limit=DEFAULT_LIMIT, max_rows=MAX_ROWS):\n",
    "    headers = {}\n",
    "    if SODA_APP_TOKEN:\n",
    "        headers['X-App-Token'] = SODA_APP_TOKEN\n",
    "    rows = []\n",
    "    offset = 0\n",
    "    pages = math.ceil(max_rows / limit)\n",
    "    for _ in range(pages):\n",
    "        q = params.copy()\n",
    "        q['$limit'] = limit\n",
    "        q['$offset'] = offset\n",
    "        url = f\"{SODA_JSON}?{urlencode(q)}\"\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        batch = r.json()\n",
    "        if not batch:\n",
    "            break\n",
    "        rows.extend(batch)\n",
    "        if len(batch) < limit:\n",
    "            break\n",
    "        offset += limit\n",
    "    return rows\n",
    "\n",
    "\n",
    "def soda_rows_to_df(rows):\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Normalize expected columns used below\n",
    "    # Map Socrata field names to those expected by the dashboard\n",
    "    # Socrata fields (lowercase) -> expected uppercase\n",
    "    col_map = {\n",
    "        'crash_date': 'CRASH_DATE',\n",
    "        'prim_contributory_cause': 'PRIM_CONTRIBUTORY_CAUSE',\n",
    "        'crash_month': 'CRASH_MONTH',\n",
    "        'crash_hour': 'CRASH_HOUR',\n",
    "        'latitude': 'LATITUDE',\n",
    "        'longitude': 'LONGITUDE',\n",
    "    }\n",
    "    for src, dst in col_map.items():\n",
    "        if src in df.columns and dst not in df.columns:\n",
    "            df[dst] = df[src]\n",
    "    return df\n",
    "\n",
    "\n",
    "if use_soda:\n",
    "    print('Loading crash data from Chicago SODA API...')\n",
    "    rows = soda_fetch_json(params)\n",
    "    df = soda_rows_to_df(rows)\n",
    "    print(f\"Loaded {len(df)} rows from SODA API\")\n",
    "else:\n",
    "    print('Loading crash data from local CSV...')\n",
    "    df = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a120331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xd/8g6_pgld5w9_rv6xb4xjxmvh0000gn/T/ipykernel_92665/4052159387.py:6: DeprecationWarning: Widget.widget_types is deprecated.\n",
      "  widgets.Widget.widget_types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipywidgets.widgets.widget.WidgetRegistry at 0x116071550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment setup without magics\n",
    "import sys, subprocess\n",
    "pkgs = ['pandas','numpy','ipywidgets','bqplot']\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
    "import ipywidgets as widgets\n",
    "widgets.Widget.widget_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b79992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xd/8g6_pgld5w9_rv6xb4xjxmvh0000gn/T/ipykernel_92665/569728642.py:4: DeprecationWarning: Widget.widget_types is deprecated.\n",
      "  widgets.Widget.widget_types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipywidgets.widgets.widget.WidgetRegistry at 0x116071550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment setup (safe to re-run)\n",
    "%pip install -q pandas numpy ipywidgets bqplot\n",
    "import ipywidgets as widgets\n",
    "widgets.Widget.widget_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93214e1",
   "metadata": {},
   "source": [
    "# Final Project Part 2 — Viz for Experts\n",
    "\n",
    "- Group Members: [Add names here]\n",
    "- Dataset: `traffic_crashes_full.csv` (loaded via relative path)\n",
    "\n",
    "This notebook implements a simple, reliable bqplot dashboard with a driver plot (crashes by month) and a driven plot (top primary crash causes) linked via selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7a694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457466d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'traffic_crashes_full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load via relative path\u001b[39;00m\n\u001b[32m     21\u001b[39m csv_path = \u001b[33m'\u001b[39m\u001b[33mtraffic_crashes_full.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Use explicit columns from dataset\u001b[39;00m\n\u001b[32m     25\u001b[39m DATE_COL = \u001b[33m'\u001b[39m\u001b[33mCRASH_DATE\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/altair-hw/Watcher-S.github.io/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/altair-hw/Watcher-S.github.io/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/altair-hw/Watcher-S.github.io/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/altair-hw/Watcher-S.github.io/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/altair-hw/Watcher-S.github.io/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'traffic_crashes_full.csv'"
     ]
    }
   ],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bqplot as bq\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Try map library (will be forced off below)\n",
    "try:\n",
    "    from ipyleaflet import Map, Heatmap, basemaps, basemap_to_tiles, DrawControl\n",
    "    MAP_OK = True\n",
    "except Exception:\n",
    "    MAP_OK = False\n",
    "\n",
    "# FORCE LOCAL MAP (disable ipyleaflet usage)\n",
    "MAP_OK = False\n",
    "\n",
    "# Ensure a DataFrame `df` is available: prefer SODA loader, else fallback to small SODA pull\n",
    "if 'df' not in globals() or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "    try:\n",
    "        from urllib.parse import urlencode\n",
    "        import math, requests\n",
    "        SODA_JSON = 'https://data.cityofchicago.org/resource/85ca-t3if.json'\n",
    "        params = {\n",
    "            '$select': '*',\n",
    "            '$order': 'crash_date DESC',\n",
    "            '$limit': 10000,\n",
    "        }\n",
    "        url = f\"{SODA_JSON}?{urlencode(params)}\"\n",
    "        rows = requests.get(url, timeout=60).json()\n",
    "        df = pd.DataFrame(rows)\n",
    "        print(f\"Loaded fallback {len(df)} rows from SODA API\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to obtain data from SODA API: {e}\")\n",
    "\n",
    "# Use explicit columns from dataset\n",
    "DATE_COL = 'CRASH_DATE'\n",
    "CAUSE_COL = 'PRIM_CONTRIBUTORY_CAUSE'\n",
    "MONTH_COL = 'CRASH_MONTH'\n",
    "HOUR_COL = 'CRASH_HOUR'\n",
    "LAT_COL = 'LATITUDE'\n",
    "LON_COL = 'LONGITUDE'\n",
    "\n",
    "# If SODA provided lowercase names, map them to expected names\n",
    "if DATE_COL not in df.columns and 'crash_date' in df.columns:\n",
    "    df[DATE_COL] = df['crash_date']\n",
    "if CAUSE_COL not in df.columns and 'prim_contributory_cause' in df.columns:\n",
    "    df[CAUSE_COL] = df['prim_contributory_cause']\n",
    "if MONTH_COL not in df.columns and 'crash_month' in df.columns:\n",
    "    df[MONTH_COL] = df['crash_month']\n",
    "if HOUR_COL not in df.columns and 'crash_hour' in df.columns:\n",
    "    df[HOUR_COL] = df['crash_hour']\n",
    "if LAT_COL not in df.columns and 'latitude' in df.columns:\n",
    "    df[LAT_COL] = df['latitude']\n",
    "if LON_COL not in df.columns and 'longitude' in df.columns:\n",
    "    df[LON_COL] = df['longitude']\n",
    "\n",
    "# Parse dates, drop unknowns\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce', infer_datetime_format=True)\n",
    "df = df.dropna(subset=[DATE_COL])\n",
    "df['YEAR'] = df[DATE_COL].dt.year\n",
    "\n",
    "# Month\n",
    "if MONTH_COL in df.columns:\n",
    "    df['MONTH'] = pd.to_numeric(df[MONTH_COL], errors='coerce')\n",
    "else:\n",
    "    df['MONTH'] = df[DATE_COL].dt.month\n",
    "\n",
    "# Hour\n",
    "if HOUR_COL in df.columns:\n",
    "    df['HOUR'] = pd.to_numeric(df[HOUR_COL], errors='coerce').clip(0,23).fillna(0).astype(int)\n",
    "else:\n",
    "    df['HOUR'] = df[DATE_COL].dt.hour.fillna(0).astype(int)\n",
    "\n",
    "# Normalize cause\n",
    "if CAUSE_COL in df.columns:\n",
    "    df[CAUSE_COL] = df[CAUSE_COL].fillna('Unknown').astype(str).str.strip()\n",
    "else:\n",
    "    cat_candidates = [c for c in df.columns if df[c].dtype == 'object']\n",
    "    CAUSE_COL = cat_candidates[0] if cat_candidates else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad2656",
   "metadata": {},
   "source": [
    "# How to Use This Dashboard\n",
    "\n",
    "This dashboard has two linked charts:\n",
    "- The top chart shows total crash counts by month. Click one or more bars to select months.\n",
    "- The bottom chart updates to show the top 10 primary crash causes within the selected months. If no months are selected, it shows the overall top causes.\n",
    "\n",
    "Tips:\n",
    "- Rotate your view or scroll horizontally if month labels wrap.\n",
    "- Use multiple selections to compare seasonal patterns.\n",
    "- Data loads from the Chicago SODA API (Traffic Crashes — Crashes). No local CSV is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0dded5",
   "metadata": {},
   "source": [
    "# Contextual Datasets\n",
    "\n",
    "- Chicago Traffic Volume (IDOT): https://idot.illinois.gov/transportation-system/Network-Overview/Highway-System/Traffic-Data.html — could contextualize crash counts by exposure (vehicle miles traveled) to assess risk.\n",
    "- Weather Data (NOAA, Chicago area): https://www.ncei.noaa.gov/ — weather conditions (precipitation, snow, temperature) may explain seasonal crash patterns.\n",
    "- Public Transit Ridership (CTA): https://www.transitchicago.com/performance/ — changes in ridership could relate to shifts in road usage and crash trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d11c6",
   "metadata": {},
   "source": [
    "# Data Size and Hosting Plan\n",
    "\n",
    "This notebook retrieves crash data directly from the Chicago SODA API, avoiding large local CSV files and GitHub Pages size limits.\n",
    "\n",
    "Notes:\n",
    "- SODA responses are paged and rate-limited; the loader caps rows for performance. Increase limits as needed when running locally.\n",
    "- For reproducibility, you can export filtered subsets to CSV, but the default flow uses live API data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
